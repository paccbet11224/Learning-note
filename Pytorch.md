# Pytorch学习笔记

#### 1. super().__ init__ ()有什么用？

```
super()用来调用父类(基类)的方法，__init__()是类的构造方法，
super().__init__() 就是调用父类的init方法， 同样可以使用super()去调用父类的其他方法。
```

#### 2. unsqueeze

> `unsqueeze(tensor, dim)` 就是在张量 `tensor` 的 **第 `dim` 维插入一个维度为 1 的轴**，从而改变 shape，但不改变数据。

------

##### 基本语法

```
new_tensor = tensor.unsqueeze(dim)
```

- `dim`：插入的位置（支持负数，比如 -1 表示最后一维）

------

##### 举例说明

##### 示例张量

```
x = torch.tensor([1, 2, 3])   # shape: [3]
```

##### 加一个维度

```
x.unsqueeze(0)  # shape: [1, 3]
x.unsqueeze(1)  # shape: [3, 1]
```

#### 3. 张量切片

**(1)a[:2]:**取第一个维度的前2个维度数据(不包括2)；
**(2)a[:2,:1,:,:]：**取第一个维度的前两个数据，取第2个维度的前1个数据，后两个维度全都取到；
**(3)a[:2,1:,:,:]：**取第一个维度的前两个数据，取第2个维度的第1个索引到最后索引的数据(包含1)，后两个维度全都取到；
**(4)a[:2,-3:]：**负号表示第2个维度上从倒数第3个数据取到最后倒数第一个数据-1(包含-3)；

##### 语法结构：

```
[:, 0::2]
```

这是一个 **二维张量的切片操作**，可以理解为：

```
取所有行，列从第 0 列开始，每隔 2 个取一个
```

------

##### 分别来看：

##### `:` → 行方向

- 表示“取所有行”
- 即不管有多少行，全都保留

##### `0::2` → 列方向

- `0`: 从第 0 列开始
- `::2`: 每隔两个取一个（步长是 2）
- 所以这会选中第 0、2、4、6...列

#### 4. nn.Linear

##### `nn.Linear` 在 PyTorch 中到底构造了什么？

```
nn.Linear(in_features, out_features)
```

会构造一个 **“线性变换层”**，数学上对应这个公式：

![image-20250627154546436](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20250627154546436.png)

其中：

- `X` 是输入张量，shape 通常是 `[batch_size, ..., in_features]`
- `W` 是权重矩阵，shape 是 `[out_features, in_features]`
- `b` 是偏置项，shape 是 `[out_features]`

##### 本质上，它构造了两个参数：

##### 1. `self.weight`: 权重矩阵

- shape: `[out_features, in_features]`
- 用于对输入做线性变换

##### 2. `self.bias`: 偏置向量

- shape: `[out_features]`
- 会加在变换结果上（默认启用）

------

##### 举个例子

```
layer = nn.Linear(4, 2)
```

你创建了一个线性层，输入是 4 维向量，输出是 2 维。

那么它内部就会自动初始化：

- `weight`: `[2, 4]`
- `bias`: `[2]`

输入一个 `[batch_size, 4]` 的张量 `x`，执行：

```
y = layer(x)
```

等价于：

```
y = x @ weight.T + bias
```